{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMS Spam Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\haris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data\n",
    "\n",
    "**Read the SMSSpamCollections file and set it as a dataframe called sms.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms = pd.read_csv('SMSSpamCollections', sep='\\t', names=['label', 'text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Check the head, info , and describe methods on sms.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5572 entries, 0 to 5571\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   label   5572 non-null   object\n",
      " 1   text    5572 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 87.2+ KB\n"
     ]
    }
   ],
   "source": [
    "sms.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5572</td>\n",
       "      <td>5572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2</td>\n",
       "      <td>5169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>ham</td>\n",
       "      <td>Sorry, I'll call later</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>4825</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label                    text\n",
       "count   5572                    5572\n",
       "unique     2                    5169\n",
       "top      ham  Sorry, I'll call later\n",
       "freq    4825                      30"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a new column called \"text length\" which is the number of words in the text column.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms['text length'] = sms['text'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Frequency'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sms['text length'].plot(bins=100, kind='hist') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    5572.000000\n",
       "mean       80.489950\n",
       "std        59.942907\n",
       "min         2.000000\n",
       "25%        36.000000\n",
       "50%        62.000000\n",
       "75%       122.000000\n",
       "max       910.000000\n",
       "Name: text length, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms['text length'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a text processing function to remove unnecessary words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_process(mess):\n",
    "    \"\"\"\n",
    "    Takes in a string of text, then performs the following:\n",
    "    1. Remove all punctuation\n",
    "    2. Remove all stopwords\n",
    "    3. Returns a list of the cleaned text\n",
    "    \"\"\"\n",
    "    # Check characters to see if they are in punctuation\n",
    "    nopunc = [char for char in mess if char not in string.punctuation]\n",
    "\n",
    "    # Join the characters again to form the string.\n",
    "    nopunc = ''.join(nopunc)\n",
    "    \n",
    "    # Now just remove any stopwords\n",
    "    return ' '.join([word for word in nopunc.split() if word.lower() not in stopwords.words('english')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms['text'] = sms['text'].apply(text_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>text length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go jurong point crazy Available bugis n great ...</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar Joking wif u oni</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry 2 wkly comp win FA Cup final tkts 2...</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say early hor U c already say</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah dont think goes usf lives around though</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text  text length\n",
       "0   ham  Go jurong point crazy Available bugis n great ...          111\n",
       "1   ham                            Ok lar Joking wif u oni           29\n",
       "2  spam  Free entry 2 wkly comp win FA Cup final tkts 2...          155\n",
       "3   ham                U dun say early hor U c already say           49\n",
       "4   ham        Nah dont think goes usf lives around though           61"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Classification Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create two objects X and y. X will be the 'text' column of sms and y will be the 'label' column of sms. (Your features and target/labels)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sms['text'].copy()\n",
    "y = sms['label'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import CountVectorizer and create a CountVectorizer object.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer().fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Use the fit_transform method on the CountVectorizer object and pass in X (the 'text' column). Save this result by overwriting X.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cv.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split\n",
    "\n",
    "Let's split our data into training and testing data.\n",
    "\n",
    "** Use train_test_split to split up the data into X_train, X_test, y_train, y_test. Use test_size=0.3 and random_state=101 **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Model\n",
    "\n",
    "Time to train a model!\n",
    "\n",
    "** Import MultinomialNB and create an instance of the estimator and call is nb **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now fit nb using the training data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions and Evaluations\n",
    "\n",
    "Time to see how our model did!\n",
    "\n",
    "**Use the predict method off of nb to predict labels from X_test.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = nb.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Create a confusion matrix and classification report using these predictions and y_test **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1446   29]\n",
      " [  15  182]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.99      0.98      0.99      1475\n",
      "        spam       0.86      0.92      0.89       197\n",
      "\n",
      "    accuracy                           0.97      1672\n",
      "   macro avg       0.93      0.95      0.94      1672\n",
      "weighted avg       0.97      0.97      0.97      1672\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, predictions))\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Great! Let's see what happens if we try to include TF-IDF to this process using a pipeline.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Text Processing\n",
    "\n",
    "** Import TfidfTransformer from sklearn. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Import Pipeline from sklearn. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Now create a pipeline with the following steps:CountVectorizer(), TfidfTransformer(),MultinomialNB()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('count_vectorize', CountVectorizer(analyzer=text_process)), \n",
    "    ('TfIdf_tranform', TfidfTransformer()), \n",
    "    ('classification', MultinomialNB())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Pipeline\n",
    "\n",
    "**Time to use the pipeline! Remember this pipeline has all your pre-process steps in it already, meaning we'll need to re-split the original data (Remember that we overwrote X as the CountVectorized version. What we need is just the text**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split\n",
    "\n",
    "**Redo the train test split on the sms object.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sms['text']\n",
    "y = sms['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now fit the pipeline to the training data. Remember you can't use the same training data as last time because that data has already been vectorized. We need to pass in just the text and labels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('count_vectorize',\n",
       "                 CountVectorizer(analyzer=<function text_process at 0x0000026B1F9298C8>)),\n",
       "                ('TfIdf_tranform', TfidfTransformer()),\n",
       "                ('classification', MultinomialNB())])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions and Evaluation\n",
    "\n",
    "** Now use the pipeline to predict from the X_test and create a classification report and confusion matrix. You should notice strange results.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_pred = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1443   32]\n",
      " [  27  170]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.98      0.98      0.98      1475\n",
      "        spam       0.84      0.86      0.85       197\n",
      "\n",
      "    accuracy                           0.96      1672\n",
      "   macro avg       0.91      0.92      0.92      1672\n",
      "weighted avg       0.97      0.96      0.96      1672\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, pipe_pred))\n",
    "print(classification_report(y_test, pipe_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
